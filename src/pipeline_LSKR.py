import os
import json
import argparse
from tqdm import tqdm
from attrdict import AttrDict
import torch
from torch.nn.functional import softmax
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers.modeling_utils import PreTrainedModel
import torch.nn as nn
from openai import OpenAI

# KG Triple ì¶”ì¶œ ëª¨ë“ˆ
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.kg2subgraph.label2triple import get_treatment_triple_from_condition
# RAG ëª¨ë“ˆ
from src.RAG.Run_RAG import get_treatment_context_from_triple

# strategy_info.json íŒŒì¼ ê²½ë¡œ ì„¤ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
current_dir = "./src/"
strategy_file_path = os.path.join(current_dir, "strategy_info.json")

try:
    with open(strategy_file_path, "r", encoding="utf-8") as f:
        strategy_info = json.load(f)
    print(f"** Strategy info loaded from: {strategy_file_path}")
except Exception as e:
    print(f"X Error loading strategy_info.json: {e}")
    # ê¸°ë³¸ê°’ ì„¤ì •
    strategy_info = {
        "depression": "Depression counseling strategy not available.",
        "anxiety": "Anxiety counseling strategy not available.",
        "bipolar": "Bipolar counseling strategy not available.",
        "Eating_disorder": "Eating disorder counseling strategy not available."
    }



# --------------------------------------------------------------------------------
# 1. ChatGPT API í˜¸ì¶œì„ ìœ„í•œ í•¨ìˆ˜ (ì²« ë²ˆì§¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê°€ì ¸ì˜´)
# --------------------------------------------------------------------------------
def create_chat_completion(client, system_prompt, user_prompt, model="gpt-4o", temperature=0.8, max_tokens=650):
    """
    OpenAIì˜ Chat Completion APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜.
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        # ì‘ë‹µì´ ìœ íš¨í•œì§€ í™•ì¸ í›„ content ë°˜í™˜
        if response and response.choices:
            return response.choices[0].message.content.strip()
        else:
            print("Warning: Invalid response structure from API.")
            return "Error: Invalid response from API."
    except Exception as e:
        print(f"Error during API call: {e}")
        return f"Error: {str(e)}"

# --------------------------------------------------------------------------------
# 2. RoBERTa ë¶„ë¥˜ ëª¨ë¸ ê´€ë ¨ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# --------------------------------------------------------------------------------

# ì§ˆí™˜ëª… ë§¤í•‘ (RoBERTa ë¶„ë¥˜ ê²°ê³¼ â†’ KG ê²€ìƒ‰ìš©)
CONDITION_MAPPING = {
    'anxiety': 'anxiety',
    'bipolar': 'bipolar',  # Bipolar Disorder, manic depressionìœ¼ë¡œ ê²€ìƒ‰
    'depression': 'depression',
    'Eating_disorder': 'Eating_disorder',  # ëŒ€ë¬¸ì E ë²„ì „ë„ ëŒ€ë¹„
    'control': 'control'  # ì¼ë°˜ ìƒë‹´
}

class MentalROBERTA_SENTIMENT_CLASSIFIER(PreTrainedModel):
    def __init__(self, config):
        super(MentalROBERTA_SENTIMENT_CLASSIFIER, self).__init__(config)
        self.roberta = AutoModel.from_pretrained(config._name_or_path)
        self.hidden_size = config.hidden_size
        self.num_labels = config.num_labels
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(self.hidden_size, self.hidden_size)
        self.linear2 = nn.Linear(self.hidden_size, self.num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        cls_vector = outputs.last_hidden_state[:, 0, :]
        linear_output = self.linear1(cls_vector)
        cls_output = self.linear2(linear_output)
        return cls_output

def load_roberta_model(model_dir):
    try:
        roberta_config = AutoConfig.from_pretrained(model_dir)
        setattr(roberta_config, "num_labels", 5)
        setattr(roberta_config, "hidden_size", 768)
        model = MentalROBERTA_SENTIMENT_CLASSIFIER.from_pretrained(
            model_dir, config=roberta_config
        )
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        return model, tokenizer, device
    except Exception as e:
        raise Exception(f"RoBERTa ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def predict_label(model, tokenizer, device, sentence):
    try:
        inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True, max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(inputs['input_ids'], inputs['attention_mask'])
            probs = softmax(outputs, dim=-1)
            pred = torch.argmax(probs, dim=-1).item()
        return pred
    except Exception as e:
        raise Exception(f"ë ˆì´ë¸” ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# --------------------------------------------------------------------------------
# 3. ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ (LLaMA í˜¸ì¶œ ë¶€ë¶„ì„ ChatGPT í˜¸ì¶œë¡œ ë³€ê²½)
# --------------------------------------------------------------------------------
def main(cli_args):
    try:
        args = AttrDict(vars(cli_args))
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = OpenAI(api_key=args.api_key)
        
        # RoBERTa ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ê³¼ ë™ì¼)
        roberta_model, roberta_tokenizer, device = load_roberta_model("./src/MentalRoBERTa/0421")

        # ë ˆì´ë¸” ë§¤í•‘ ë¡œë“œ
        # with open("./src/MentalRoBERTa/0421/label2idx.json", 'r', encoding='utf-8') as f:
        #     label2idx = json.load(f)
        # idx2label = {str(v): k for k, v in label2idx.items()}
        # ë¼ë²¨ ë§¤í•‘ ë¡œë“œ (ì˜¬ë°”ë¥¸ ë²„ì „)
        with open("./src/MentalRoBERTa/0421/label2idx.json", 'r', encoding='utf-8') as f:
            label_mapping = json.load(f)
        label2idx = label_mapping["label2idx"]  # ì¤‘ì²©ëœ êµ¬ì¡°ì—ì„œ ì¶”ì¶œ
        idx2label = {str(v): k for k, v in label2idx.items()}

        # PsyQA ë²ˆì—­ ë°ì´í„° ë¡œë“œ (ì²˜ìŒ 1000ê°œë§Œ)
        print("Loading translated PsyQA dataset...")
        with open(args.data_file, 'r', encoding='utf-8') as f:
            full_dataset = json.load(f)
            
        # 400ê°œ ìƒ˜í”Œ íŒŒì¼ì„ ì „ì²´ ì‚¬ìš©
        dataset = full_dataset
        
        print(f"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {len(full_dataset)}")
        print(f"ì²˜ë¦¬í•  ë°ì´í„° í¬ê¸°: {len(dataset)} (400ê°œ ìƒ˜í”Œ)")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(args.output_dir, exist_ok=True)
        # ì¶œë ¥ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í†µì¼í•˜ì—¬ ê°„ê²°í•˜ê²Œ ë§Œë“¦
        output_path = os.path.join(args.output_dir, args.output_file)

        # ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì¤‘ê°„ë¶€í„° ì¬ì‹œì‘
        processed_ids = set()
        start_idx = 0
        
        if os.path.exists(output_path):
            print(f"@@ ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ë°œê²¬: {output_path}")
            try:
                with open(output_path, 'r', encoding='utf-8') as existing_file:
                    for line_num, line in enumerate(existing_file):
                        if line.strip():
                            try:
                                result = json.loads(line.strip())
                                processed_ids.add(result.get("questionID", ""))
                            except json.JSONDecodeError:
                                print(f"!! ì¤„ {line_num+1}ì—ì„œ JSON íŒŒì‹± ì˜¤ë¥˜, ë¬´ì‹œí•©ë‹ˆë‹¤.")
                                continue
                
                start_idx = len(processed_ids)
                print(f"** ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œ: {len(processed_ids)}ê°œ")
                print(f"ì¸ë±ìŠ¤ {start_idx}ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"!! ê¸°ì¡´ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                print("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
                processed_ids = set()
                start_idx = 0

        # íŒŒì¼ ëª¨ë“œ ê²°ì • (append ë˜ëŠ” write)
        file_mode = 'a' if start_idx > 0 else 'w'
        
        with open(output_path, file_mode, encoding='utf-8') as of:
            # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
            remaining_dataset = dataset[start_idx:] if start_idx > 0 else dataset
            progress_desc = f"Processing samples ({start_idx+1}-{len(dataset)}) with real-time RAG" if start_idx > 0 else f"Processing {len(dataset)} samples with real-time RAG"
            
            for relative_idx, data in enumerate(tqdm(remaining_dataset, desc=progress_desc, initial=start_idx, total=len(dataset))):
                try:
                    # ì‹¤ì œ ì¸ë±ìŠ¤ ê³„ì‚° (ì „ì²´ ë°ì´í„°ì…‹ì—ì„œì˜ ìœ„ì¹˜)
                    actual_idx = start_idx + relative_idx
                    
                    # PsyQA ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ í•„ë“œ ì¶”ì¶œ
                    question_id = data.get("idx", "")  # ìƒˆë¡œ ì¶”ê°€ëœ idx ì‚¬ìš©
                    
                    # ì´ë¯¸ ì²˜ë¦¬ëœ IDì¸ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                    if question_id in processed_ids:
                        print(f"â­ ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œ ê±´ë„ˆëœ€: {question_id}")
                        continue
                    
                    question_title = data.get("question", "")  # ë²ˆì—­ëœ ì§ˆë¬¸
                    question_desc = data.get("description", "")  # ë²ˆì—­ëœ ì„¤ëª…
                    answers = data.get("answers", [])
                    answer_text = answers[0].get("answer_text", "") if answers else ""

                    # ì§ˆë¬¸ê³¼ ì„¤ëª…ì„ í•©ì³ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
                    post = f"{question_title}. {question_desc}"

                    # 1. RoBERTaë¡œ ë ˆì´ë¸” ì˜ˆì¸¡ (ê¸°ì¡´ê³¼ ë™ì¼)
                    pred_label_idx = predict_label(roberta_model, roberta_tokenizer, device, post)
                    pred_label = idx2label.get(str(pred_label_idx), "unknown")
                    # strategy = strategy_info.get(pred_label, "No strategy available.")

                    # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬)
                    if pred_label == "control":
                        system_prompt = (
                            "You are a helpful and insightful general counselor. Your goal is to provide a thoughtful "
                            "perspective and practical advice for everyday life concerns and general questions. "
                            "Your tone should be supportive and encouraging, but not clinical."
                            "Please provide a comprehensive, cohesive, and empathetic response of approximately 650 tokens that flows naturally, avoids list formats, and maintains an emotionally supportive tone throughout."
                        )
                        user_prompt = (
                            f"The user's counseling content is as follows: {post}\n\n"
                            f"Based on my analysis, the category of this concern is 'General/Control', "
                            "As an advisor, an appropriate approach to support the user includes the following principles: The foundation is active and empathetic listening, which helps the user feel heard and understood. It is important to validate their feelings without judgment, creating a safe space for them to express themselves. Encouraging gentle self-reflection can help them gain clarity on their own. Finally, offering encouragement to consider small, positive steps forward can be empowering, without being prescriptive.\n\n"
                            "Please generate a compassionate, conversational, and well-structured response that reflects the user's counseling content with empathy and emotional support. "
                            "Naturally integrate the predicted mental disorder, the recommended treatment strategy, and the medication information, ensuring the response flows smoothly in a natural, non-list format, with a warm and encouraging tone."
                        )
                    else: # ì •ì‹  ì§ˆí™˜ ë¼ë²¨ì„ ìœ„í•œ ì „ë¬¸ ìƒë‹´ í”„ë¡¬í”„íŠ¸ + KG Triple & RAG ì½˜í…ì¸ 
                        # 3. KG Triple ì¶”ì¶œ
                        print(f"ğŸ” [{actual_idx+1}/{len(dataset)}] {pred_label} ì§ˆí™˜ì— ëŒ€í•œ ì¹˜ë£Œë²• Triple ê²€ìƒ‰ ì¤‘...")
                        mapped_condition = CONDITION_MAPPING.get(pred_label, pred_label)
                        triple_result = get_treatment_triple_from_condition(
                            condition=mapped_condition,
                            user_query=post,
                            neo4j_uri="bolt://localhost:7687",
                            neo4j_user="neo4j", 
                            neo4j_password="dkahdkah10"
                        )
                        
                        # 4. ì§ˆí™˜ë³„ ì¹˜ë£Œ ì „ëµ ê°€ì ¸ì˜¤ê¸°
                        counseling_strategy = strategy_info.get(pred_label, strategy_info.get("depression", ""))  # ê¸°ë³¸ê°’ìœ¼ë¡œ depression ì „ëµ ì‚¬ìš©
                        
                        # 5. RAG ì½˜í…ì¸  ìƒì„± (ì‹¤ì‹œê°„ ì²˜ë¦¬)
                        print(f"ğŸ“š [{actual_idx+1}/{len(dataset)}] ì¹˜ë£Œë²• ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ ë° ì½˜í…ì¸  ìƒì„± ì¤‘...")
                        
                        # RAG ì²˜ë¦¬ë¥¼ í†µí•´ ë…¼ë¬¸ ì •ë³´ì™€ ìƒì„¸ ê²°ê³¼ ëª¨ë‘ ì–»ê¸°
                        from src.RAG.Run_RAG import EnhancedRAGSystem
                        
                        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ìƒì„¸ ê²°ê³¼ ì–»ê¸°
                        rag_system = EnhancedRAGSystem(margin_delta=0.03)
                        detailed_rag_result = rag_system.process_single_triple(
                            disease=triple_result['selected_triple']['start_node']['name'],
                            relation=triple_result['selected_triple']['relation'],
                            drug=triple_result['selected_triple']['end_node']['name'],
                            user_query=post,
                            condition=mapped_condition
                        )
                        
                        # ê°„ë‹¨í•œ drug_treatment_info ìƒì„± (GPT í”„ë¡¬í”„íŠ¸ìš©)
                        if detailed_rag_result["success"]:
                            paper = detailed_rag_result["best_paper"]
                            drug_treatment_info = (
                                f"**Medication: {triple_result['selected_triple']['end_node']['name']}**\n"
                                f"Title: {paper['title']}\n"
                                f"Abstract: {paper['abstract']}"
                            )
                        else:
                            drug_treatment_info = "Professional medication evaluation and treatment should be considered as part of comprehensive care."
                        
                        system_prompt = (
                            "You are a compassionate and empathetic mental health counseling expert. "
                            "Carefully analyze the user's counseling content and respond with warmth, understanding, and non-judgment."
                            "Your response should be supportive and comforting while still being evidence-based and tailored to the user's predicted disorder, recommended strategy, and medication information."
                            "Please provide a comprehensive, cohesive, and empathetic response of approximately 650 tokens that flows naturally, avoids list formats, and maintains an emotionally supportive tone throughout."

                        )
                        user_prompt = (
                            f"The user's counseling content is: {post}\n"
                            f"The predicted user's mental disorder information is : {pred_label}\n"
                            f"the Recommended treatment strategy for this condition is: {counseling_strategy}\n"
                            f"And the Relevant medication treatment information is: {drug_treatment_info}\n"
                            "Please generate a compassionate, conversational, and well-structured response that reflects the user's counseling content with empathy and emotional support. "
                            "Naturally integrate the predicted mental disorder, the recommended treatment strategy, and the medication information, ensuring the response flows smoothly in a natural, non-list format, with a warm and encouraging tone."
                        )

                            # "Please generate a response that addresses the user's counseling content while effectively incorporating the mental disorder information, treatment strategy, and medication details."
                        # "Please generate a response that addresses the user's counseling content while effectively incorporating the mental disorder information, treatment strategy, and medication details."
                            # "Please generate a response that reflects the user's counseling content and the predicted mental disorder, incorporating the above specialized strategies and evidence-based medication information. "
                            # "The response should be empathetic, supportive, and practical, while being specific to their diagnosed condition."                        
                            # "response aimed at treating the user's mental disorder by utilizing the user's counseling content, mental disorder information, and the corresponding treatment strategy and Medication Treatment Information: "
                    # 3. ChatGPT API í˜¸ì¶œë¡œ í…ìŠ¤íŠ¸ ìƒì„±
                    generated = create_chat_completion(client, system_prompt, user_prompt, model="gpt-4o", max_tokens=650)

                    # 5. ê²°ê³¼ ì €ì¥
                    # JSON ì €ì¥ì„ ìœ„í•´ ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ì ì ˆíˆ ì²˜ë¦¬
                    result = {
                        "questionID": question_id,
                        "total_questionText": post.replace('\n', ' ').replace('\r', ' '),  # ì¤„ë°”ê¿ˆ ì œê±°
                        "model_answer": generated.replace('\n', ' ').replace('\r', ' '),   # ì¤„ë°”ê¿ˆ ì œê±°
                        "answerText": answer_text.replace('\n', ' ').replace('\r', ' '),   # ì¤„ë°”ê¿ˆ ì œê±°
                        "predicted_label": pred_label
                    }
                    
                    # ì •ì‹ ì§ˆí™˜ ë¶„ë¥˜ì˜ ê²½ìš° Triple ë° RAG ì •ë³´ ì¶”ê°€
                    if pred_label != "control" and 'triple_result' in locals() and triple_result['status'] == 'success':
                        result["kg_triple_info"] = {
                            "condition": triple_result['condition'],
                            "selected_treatment": triple_result['summary']['selected_treatment'],
                            "selected_start_node": triple_result['summary']['selected_start_node'],
                            "relation": triple_result['summary']['relation']
                        }
                        
                        # RAG ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¶”ê°€
                        if 'detailed_rag_result' in locals() and detailed_rag_result["success"]:
                            best_paper = detailed_rag_result["best_paper"]
                            result["rag_paper_info"] = {
                                "pmid": best_paper.get("pmid", ""),
                                "title": best_paper.get("title", ""),
                                "abstract": best_paper.get("abstract", ""),
                                "journal": best_paper.get("journal", ""),
                                "year": best_paper.get("year", ""),
                                "mesh_terms": best_paper.get("mesh_terms", []),
                                "publication_types": best_paper.get("publication_types", []),
                                "clinical_score": best_paper.get("clinical_score", 0.0)
                            }
                            
                            # RAG ì²˜ë¦¬ ê³¼ì • ì •ë³´ ì¶”ê°€
                            result["rag_process_info"] = {
                                "selection_method": detailed_rag_result["selection_info"]["selection_method"],
                                "pubmed_query": detailed_rag_result["selection_info"]["pubmed_query"],
                                "pipeline_stats": detailed_rag_result["pipeline_stats"]
                            }
                        else:
                            result["rag_paper_info"] = None
                            result["rag_process_info"] = {
                                "selection_method": "failed",
                                "reason": detailed_rag_result.get("reason", "unknown") if 'detailed_rag_result' in locals() else "no_rag_attempted"
                            }
                    of.write(json.dumps(result, ensure_ascii=False) + "\n")
                    # ë§¤ ìƒ˜í”Œë§ˆë‹¤ íŒŒì¼ í”ŒëŸ¬ì‹œ (ì§„í–‰ ìƒí™© ë³´ì¡´)
                    of.flush()

                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (RoBERTa ì‚¬ìš© í›„)
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                except Exception as e:
                    print(f"X Error processing data for questionID {question_id}: {str(e)}")
                    # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒŒì¼ì„ ì¦‰ì‹œ í”ŒëŸ¬ì‹œí•˜ì—¬ ì§„í–‰ ìƒí™© ë³´ì¡´
                    of.flush()
                    continue

        print(f"Results successfully saved to {output_path}")

    except Exception as e:
        print(f"An error occurred in the main execution: {str(e)}")
        raise

if __name__ == "__main__":
    cli_parser = argparse.ArgumentParser(description="Run inference using RoBERTa for classification and ChatGPT for generation.")
    
    # í•„ìˆ˜ ì¸ì
    cli_parser.add_argument("--api_key", type=str, default="", help="Your OpenAI API key.")
    
    # íŒŒì¼ ê²½ë¡œ ì¸ì
    # cli_parser.add_argument("--data_file", type=str, default="./src/MentalRoBERTa/0421/v2_finaldf123_inference_sample.jsonl", help="Path to the input data file.")
    cli_parser.add_argument("--data_file", type=str, default="data/main/400_psyqa_translated.json", help="Path to the 400-sample PsyQA data file.")
    cli_parser.add_argument("--output_dir", type=str, default="./output_dir/pipeline", help="Directory to save the output file.")
    cli_parser.add_argument("--output_file", type=str, default="gpt35_psyqa_inference_LSKR.jsonl", help="Name of the output file.")
    
    args = cli_parser.parse_args()
    
    # API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•˜ì§€ë§Œ, ì¸ìë¡œ ì§ì ‘ ì „ë‹¬
    # os.environ["OPENAI_API_KEY"] = args.api_key
    
    main(args)